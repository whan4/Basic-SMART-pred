---
title: "ML_pipeline"
author: "Wenzheng Han"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load necessary package

Install and load the renv package:

```{r renv_setup, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# if (!requireNamespace("renv", quietly = TRUE)) {
#   install.packages("renv")
# }
# library("renv")
# 
# # Initialize the renv environment
# # renv::init()
# renv::restore()
```

# Install and Load Packages

```{r install_packages, echo=FALSE, warning=FALSE}
# Define packages list
pkgs <- c(
  "ada",
  "caret",
  "doParallel",
  "dplyr",
  "e1071",
  "gbm",
  "ggplot2",
  "klaR",
  "kernlab",
  "Metrics",
  "MLmetrics",
  "naivebayes",
  "plotROC",
  "pROC",
  "randomForest",
  "recipes",
  "reshape2",
  "rpart",
  "rsample",
  "tidyr",
  "tibble",
  "xgboost"
)

# Install packages
for (pkg in pkgs) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
}

# # Record dependencies
# renv::snapshot()

# Load packages
for (pkg in pkgs) {
  library(pkg, character.only = TRUE)
}
```

# Input Data

```{r load_data, echo=FALSE}
# Load data
data <- read.csv("~/Desktop/ML_pipeline/data/DARWIN.csv", header = TRUE)

# Preprocess data
data$class <- as.factor(data$class)
```

# Stratified Sampling

```{r Stratified_Sampling, echo=FALSE}
set.seed(123)
split <- initial_split(data, prop = 0.7, strata = "class")
train <- training(split)
test <- testing(split)
```

# PCA and Selection of Principal Components

```{r pca_selection, echo=FALSE}
# Perform PCA and select appropriate PCs
pca_model <- prcomp(train[, sapply(train, is.numeric)], scale. = TRUE)
explained_variance <- pca_model$sdev^2 / sum(pca_model$sdev^2)
cumulative_variance <- cumsum(explained_variance)
num_pcs <- which(cumulative_variance >= 0.95)[1]

# Print the number of principal components to retain
cat("Number of principal components to retain for 95% variance:", num_pcs, "\n")

# Calculate eigenvalues
eigenvalues <- pca_model$sdev^2

# Select PCs with eigenvalues greater than 1
kaiser_pcs <- sum(eigenvalues > 1)
print(paste("Number of principal components to retain using Kaiser Criterion:", kaiser_pcs))
```

# Preprocess Data

```{r Preprocess Data, echo=FALSE}
# Remove ID column
train <- train[, setdiff(names(train), c("ID"))]
test <- test[, setdiff(names(train), c("ID"))]

# Set blueprint of preprocess
blueprint <- recipe(class ~ ., data = train) %>%
       step_impute_mean(all_numeric_predictors()) %>%  
       step_impute_mode(all_nominal_predictors()) %>%  
       step_nzv(all_predictors()) %>%  
       step_corr(all_numeric_predictors(), threshold = 0.9) %>% 
       step_center(all_numeric_predictors()) %>%  
       step_scale(all_numeric_predictors())  %>%
       step_pca(all_numeric(), -all_outcomes(), num_comp =kaiser_pcs)
```

## If run Variable importance then run below

```{r Preprocess Data VImp, echo=FALSE}
# Remove ID column
train <- train[, setdiff(names(train), c("ID"))]
test <- test[, setdiff(names(train), c("ID"))]

# Set blueprint of preprocess
blueprint <- recipe(class ~ ., data = train) %>%
       step_impute_mean(all_numeric_predictors()) %>%  
       step_impute_mode(all_nominal_predictors()) %>%  
       step_nzv(all_predictors()) %>%  
       step_corr(all_numeric_predictors(), threshold = 0.9) %>% 
       step_center(all_numeric_predictors()) %>%  
       step_scale(all_numeric_predictors())  %>%
       step_pca(all_numeric(), -all_outcomes(), num_comp =kaiser_pcs)
```

# Train, Tune, and Evaluate the Models

## Create a trainControl object

```{r trainControl, echo=FALSE}
cv <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 5,
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  savePredictions = TRUE
)
# Initialize a list to store results
results <- list()
```

## Enable Parallel Processing

```{r parallel processing, echo=FALSE}
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
```

## Training

### Logistic regresssion

```{r Logistic_regresssion, warning=FALSE, include=FALSE}
set.seed(123)
model <- train(
  blueprint, 
  data = train,
  method = "glm",
  family = binomial,
  trControl = cv,
  metric = "ROC"
)
results[["logistic_regression"]] <- model
```

### Naive Bayes

```{r Naive_Bayes, warning=FALSE, include=FALSE}
set.seed(123)
model <- train(
  blueprint, 
  data = train,
  method = "naive_bayes",
  trControl = cv,
  metric = "ROC"
)
results[["naive_bayes"]] <- model
```

### KNN

```{r KNN, warning=FALSE, include=FALSE}
search_grid <- expand.grid(
  k = floor(seq(1, nrow(train)/3, length.out = 20))
)

set.seed(123)
model <- train(
  blueprint, 
  data = train,
  method = "knn",
  trControl = cv,
  tuneGrid = search_grid,
  metric = "ROC"
)
results[["knn"]] <- model
```

### Decision Tree

```{r Decision Tree, warning=FALSE, include=FALSE}
search_grid <- expand.grid(cp = seq(0.001, 0.1, by = 0.01))
set.seed(123)
model <- train(
  blueprint, 
  data = train,
  method = "rpart",
  trControl = cv,
  tuneGrid = search_grid,
  metric = "ROC"
)
results[["decision_tree"]] <- model
```

### Random Forest

```{r Random Forest, warning=FALSE, include=FALSE}
search_grid <- expand.grid(mtry = seq(2, 10, by = 1))
set.seed(123)
model <- train(
  blueprint, 
  data = train,
  method = "rf",
  trControl = cv,
  tuneGrid = search_grid,
  metric = "ROC"
)
results[["random_forest"]] <- model
```

### Gradient Boosting

```{r Gradient Boosting, warning=FALSE, include=FALSE}
search_grid <- expand.grid(interaction.depth = c(1, 3, 5), 
                           n.trees = seq(50, 150, by = 50), 
                           shrinkage = c(0.01, 0.1),
                           n.minobsinnode = 10)
set.seed(123)
model <- train(
  blueprint, 
  data = train,
  method = "gbm",
  trControl = cv,
  tuneGrid = search_grid,
  metric = "ROC"
)
results[["graident_boosting"]] <- model
```

### Adaboost

```{r Adaboost, warning=FALSE, include=FALSE}
search_grid <- expand.grid(
  iter = seq(50, 200, by = 50),
  maxdepth = seq(1, 5),
  nu = 0.1
)
set.seed(123)
model <- train(
  blueprint, 
  data = train,
  method = "ada",
  trControl = cv,
  tuneGrid = search_grid,
  metric = "ROC"
)
results[["adaboost"]] <- model
```

### Xgboost

```{r Xgboost, message=FALSE, warning=FALSE, include=FALSE}
search_grid <- expand.grid(
  nrounds = seq(from = 100, to = 300, by = 100),
  eta = c(0.025, 0.05, 0.1, 0.3),
  max_depth = c(4, 5, 6),
  gamma = c(0, 1, 2),
  colsample_bytree = c(0.5, 0.75, 1.0),
  min_child_weight = c(1, 3, 5),
  subsample = 1
)

set.seed(123)
model <- train(
  blueprint, 
  data = train,
  method = "xgbTree",
  trControl = cv,
  tuneGrid = search_grid,
  metric = "ROC"
)
results[["xgboost"]] <- model
```

### svm

```{r svm, message=FALSE, warning=FALSE, include=FALSE}
search_grid <- expand.grid(
  sigma= 2^c(-25, -20, -15,-10, -5, 0), 
  C= 2^c(0:5)
)

set.seed(123)
model <- train(
  blueprint, 
  data = train,
  method = "svmRadial",
  trControl = cv,
  tuneGrid = search_grid,
  metric = "ROC"
)
results[["svm"]] <- model
```

### Neral Network

```{r Neral Network, message=FALSE, warning=FALSE, include=FALSE}
search_grid <- expand.grid(
   size = seq(from = 1, to = 10, by = 1),
   decay = seq(from = 0.1, to = 0.5, by = 0.1)
 )

set.seed(123)
model <- train(
  blueprint,
  data = train,
  method = "nnet",
  trControl = cv,
  tuneGrid = search_grid,
  metric = "ROC"
  )
results[["neural_network"]] <- model
```
# Extract and Evaluate Validation Performance Metrics

```{r Evaluation Performance Metrics, echo=FALSE}
# Extract performance metrics and create a dataframe
performance_metrics <- lapply(results, function(model) {
  predictions <- model$pred
  cm <- confusionMatrix(predictions$pred, predictions$obs)
  accuracy_ci_lower <- cm$overall["AccuracyLower"]  
  accuracy_ci_upper <- cm$overall["AccuracyUpper"] 
  
  # Calculate metrics
  accuracy <- Accuracy(predictions$pred, predictions$obs)
  ci <- sprintf("(%.3f-%.3f)", accuracy_ci_lower, accuracy_ci_upper)
  sensitivity <- Sensitivity(predictions$pred, predictions$obs)
  precision <- Precision(predictions$pred, predictions$obs)
  specificity <- Specificity(predictions$pred, predictions$obs)
  f1_score <- F1_Score(predictions$pred, predictions$obs)
  roc <- max(model$results$ROC, na.rm = TRUE)
  
  # Extract the best tuning parameters
  best_params <- model$bestTune
  param_str <- if (!is.null(best_params)) {
    paste(names(best_params), best_params, sep = "=", collapse = ", ")
  } else {
    ""
  }

  data.frame(
    Model = model$method,
    Accuracy = accuracy,
    `Accuracy (95% CI)` = ci,
    Recall = sensitivity,
    Precision = precision,
    Specificity = specificity, 
    F1_Score = f1_score,
    ROC = roc,
    Best_Params = param_str,
    stringsAsFactors = FALSE,
    check.names=FALSE
  )
})

performance <- do.call(rbind, performance_metrics)
# Display the dataframe
write.csv(performance, "./full_val_Metrics.csv", quote = TRUE)
print(performance)
```
# Evaluate Performance Metrics on test dataset

```{r Test performance metrics, echo=FALSE}
performance_metrics <- lapply(results, function(model) {
  predictions <- predict(model, test)
  cm <- confusionMatrix(predictions, test[, ncol(test)])
  accuracy_ci_lower <- cm$overall["AccuracyLower"]  
  accuracy_ci_upper <- cm$overall["AccuracyUpper"]
  
  # Calculate metrics
  accuracy <- Accuracy(predictions, test[, ncol(test)])
  ci <- sprintf("(%.3f-%.3f)", accuracy_ci_lower, accuracy_ci_upper)
  sensitivity <- Sensitivity(predictions, test[, ncol(test)])
  precision <- Precision(predictions, test[, ncol(test)])
  specificity <- Specificity(predictions, test[, ncol(test)])
  f1_score <- F1_Score(predictions, test[, ncol(test)])
  roc <- max(model$results$ROC, na.rm = TRUE)

  data.frame(
    Model = model$method,
    Accuracy = accuracy,
    `Accuracy (95% CI)` = ci,
    Recall = sensitivity,
    Precision = precision,
    Specificity = specificity,
    F1_Score = f1_score,
    ROC = roc,
    stringsAsFactors = FALSE,
    check.names=FALSE
  )
})

performance <- do.call(rbind, performance_metrics)

# Display the dataframe
write.csv(performance, "./full_test_Metrics.csv")
print(performance)
```
# Varible importance

```{r Varible importance, echo=FALSE}
importance_metrics <- lapply(results, function(model) {
  importance_metrics <- varImp(model, scale = T)
  importance_metrics
})

```
# Draw Varible Importance plot
```{r Draw plot, echo=FALSE}
# Standardize the column names across all models
varimp_list <- list()
for (i in 1:length(importance_metrics)) {
  # Rename "Overall" or "importance" to "importance" for consistency
  varimp_list[[i]] <- importance_metrics[[i]][[1]]
}
for (i in 1:length(importance_metrics)) {
if (all(c("H", "P") %in% names(varimp_list[[i]])) && 
    all(varimp_list[[i]]$H == varimp_list[[i]]$P)) {
  
  # If H and P are identical, remove P and rename H to Importance
  varimp_list[[i]] <- varimp_list[[i]] %>%
    dplyr::select(-P) %>%
    rename(Importance = H)
  
} else if ("Overall" %in% names(varimp_list[[i]])) {
  
  # If 'Overall' column exists, rename it to Importance
  varimp_list[[i]] <- varimp_list[[i]] %>%
    rename(Importance = Overall)
}
  varimp_list[[i]]$model <- names(importance_metrics)[i]
}

 varimp_list <- lapply(varimp_list, function(df){
   df$Variable <- rownames(df)
   df %>%  
    slice_max(order_by = Importance, n = 10, with_ties = FALSE)  
   })

 varimp_df <- bind_rows(varimp_list)
write.csv(varimp_df, "varimp_df.csv", row.names = FALSE)
reorder_within <- function(x, by, within, fun = mean, sep = "___", ...) {
    new_x <- paste(x, within, sep = sep)
    stats::reorder(new_x, by, FUN = fun)
}

scale_x_reordered <- function(..., sep = "___") {
    reg <- paste0(sep, ".+$")
    ggplot2::scale_x_discrete(labels = function(x) gsub(reg, "", x), ...)
}
 
model_labels <- setNames(  
  paste0("(", letters[1:length(unique(varimp_df$model))], ") ", names(results)),  
  names(results)  
)  

ggplot(varimp_df, aes(x = reorder_within(Variable, -Importance, model), y = Importance, fill = model)) +
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  labs(
       x = "Variable",
       y = "Importance") +
  theme_minimal(base_size = 5.5) +  
  theme(text = element_text(face = "bold"),
        axis.text.x = element_text(angle = 55, hjust = 1, size = 5.5, face = "bold"), 
        axis.text.y = element_text(size = 5.5, face = "bold"),  
        strip.text = element_text(size = 5.5, face = "bold"),  
        plot.title = element_text(hjust = 0.5, size = 7, face = "bold"),  
        legend.position = "right",
        legend.key.size = unit(0.5,"cm")) +  
  facet_wrap(~factor(model,names(results)), scales = "free", ncol = 2, labeller = as_labeller(model_labels)) 

# Save the plot with larger dimensions and higher resolution
 ggsave("./top_10_variable_importance_per_model_fatter_taller.png", width = 5.26, height =7.6, units = "in", dpi = 300)
```
# ROC curve
```{r ROC curve, echo = FALSE}
roc_curves <- lapply(results, function(model) {
  predictions <- predict(model, test, type = "prob")[, 2]
  roc <- roc(test[, ncol(test)],predictions)
})

colors <- rainbow(length(roc_curves))
png("./_roc_curves_plot.png", width = 5.26, height = 5.26, units = "in", res = 300)
plot(roc_curves[[1]], 
     col = colors[1], 
     lwd = 2,
     asp = NA,
     xlab = "False Positive Rate (FPR)",
     ylab = "True Positive Rate (TPR)",
     cex.axis = 1,
     cex.lab = 1,
     legacy.axes = TRUE) 

if (length(roc_curves) > 1) {  
  for (i in 2:length(roc_curves)) {  
    lines(roc_curves[[i]], col = colors[i], lwd = 2)  
  }  
}  

legend_text <- sapply(1:length(roc_curves), function(i) {  
  sprintf("%s (AUC: %.2f)", names(results)[i], auc(roc_curves[[i]]))  
})  

legend("bottomright",   
       col = colors,   
       lty = 1,  
       legend = legend_text,
       cex = 0.65) 

dev.off()
```

# Stop Parallel Processing

```{r stop parallel, echo = FALSE}
stopCluster(cl)
registerDoSEQ()
```